# This thought cannot be thunk

I recently found myself on the Wikipedia page entitled [List of animals by number of neurons](https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons). I was surprised to learn that starfish have about 500 neurons, only double the neurons of a microscopic tardigrade. Is that even enough neurons to have a sensation of thought? What _is_ the sensation of thought? In any case, I figured, those poor things will never experience even a fraction of the thoughts that a human can have in the span of a single day. That left me with a nagging question, though.

> Are there thoughts that no human will ever be able to think?

On one hand, thoughts are usually characterized (somewhat anthropocentrically) by the fact that someone, you know, _thought_ them. I mean, a starfish definitely has no idea of the thoughts that lay beyond its simple echinodermic existence, so how could we? Call me overconfident, but I have more faith in humans. With our self-awareness and power for abstraction, I think we can chip away at some different angles of this question and gain a vague intuition for what lies beyond. It's like looking at a black hole; there's literally nothing there to see– no photons to reach your eyes– but by observing all the stuff _around it_, we know it's there, and that's pretty cool.

So with that said, let's think the unthinkable!

## Thought and experience

We're obviously dealing with a very thorny and layered question, but just to start somewhere, let's start with something we've all experienced: experiences. While I don't claim to know the exact relationship between experiences and thoughts, allow me to fulfill my quota of "one controversial opinion per essay" by asserting that there _is_ a relationship. Jokes aside, one could argue that all thoughts are rooted, somehow, in prior experience. Immediately after writing that sentence, I fell down a very deep rabbit hole called "Would a brain that never received any stimulus experience thought?" which I'm just going to have to put aside for now. It's still safe to say that most of your thoughts _draw from_ your experiences, :hook["real"]{id="real"} or imagined.

Let's take an apple, the poster child of thinkable objects. Thinking about an apple is pretty easy. If you're a visual thinker, you can just picture an apple. Visual thinking isn't the only way to go, though. If it was, we'd have to argue that people with [aphantasia](https://en.wikipedia.org/wiki/Aphantasia) can't think of things, which is a loser take. Personally, if you asked me to think of an apple, the experience is more like the _concept_ of an apple being brought to the forefront of my mind, primed to answer questions like "what does one taste like?" or "could you build a house out of them?". I've had plenty of experiences with apples, so it's equally easy to think of apples rolling down hills, people bobbing for apples, and so on.

It goes a step further; I've never seen a blue apple, in person or otherwise, but it's just as easy to conceptualize a blue apple. Why? I've seen blue things, and I've seen apples, and I've seen things be different colors, so bada-bing, bada-boom, blue apple.

We can take this yet another step further, beyond the realm of things I haven't experienced to the realm of things no human :hook[could possibly experience]{id="possibly-experience"}, like a cow jumping over the moon, or a person walking through a wall. Impossible to think? Not in the slightest; I just did, and you did as well. I can picture a cow jumping over the moon because I've seen cows, animals jumping, and the moon. Even if I hadn't seen characters clipping through walls in video games, I could draw from my experience of liquids to imagine someone walking through a wall as if it were a vertical pool of viscous wall-like liquid.

This feels like :hook[second nature]{id="second-nature"} to us (because it is), but it is actually extremely deep. Our capability to imagine impossibilities by freely combining concepts is one of our greatest strengths. In fact, without it, we wouldn't have the rich language that sets us apart from other animals. As Professor Dr. Eric Reuland writes in his paper [_Language and imagination: Evolutionary explorations_](https://pubmed.ncbi.nlm.nih.gov/28041788/):

> Given the [linguistic] tools discussed, we have recursive combinability and principles enabling the interpretation of the structures produced. The interpretation rules are insensitive to plausibility or implausibility, sense or nonsense. They as easily combine _brown_ with _bear_ as _square_ with _circle_ or _white_ (or _black_) with _hole_. A stone may jump, a mountain may hear. In short what we have is _imagination unleashed_.

So, if our brain is capable of combining concepts to create essentially infinitely many imagined realities, what could possibly stop us?

### New ideas can't appear in a vacuum

Let's travel 10,000 years back in time to 8,000 BC and find a "bustling" agricultural civilization nestled in the [Fertile Crescent](https://en.wikipedia.org/wiki/Fertile_Crescent). You think, well, here are a bunch of humans who have brains essentially identical to ours (evolutionarily speaking) and are capable of full-fledged spoken language. Now, let's just wait for one of them to randomly synthesize enough thoughts to stumble upon the modern-day design of the internet. Just to make it a little more tractible, say we come up with a description of the internet that feels sufficiently accurate while being _theoretically_ possible to grasp by a person with 8,000 BC technology. I made an attempt for the sake of illustration; read it if you want, but it's mostly a proof of concept.

> **The internet for the Stone Age polymath**
>
> First, we must understand the idea of code. A code is essentially a way of using _symbols_ to _represent_ something. Once you understand this, you can invent _written language_, which uses an _alphabet_ (a set of symbols) to represent spoken language. Similarly with numbers to represent the quantity of things. Now, consider a message which says `erase all the letter "e"s in this message`. If you were to follow this instruction, you would modify the symbols in the message and wind up with `ras all th lttr ""s in this mssag`. This illustrates the point that symbols can be _both_ "just" symbols _and_ (once interpreted) instructions for modifying symbols.
>
> Once you grasp this concept, you can begin to conceptualize a Turing Machine (which is better described [elsewhere](https://www.quantamagazine.org/alan-turings-most-important-machine-was-never-built-20230503/), but which doesn't require any fancy mathematics or material science to imagine). Once you've got the Turing Machine down, you can start to imagine that you have a physical material that allows you to represent symbols; for example, a magical material that can be precisely controlled to have two states: an "off" and "on" state. We'll call one chunk of this material a _bit_. Just like the "e" example, these bits can be used both as data (for example, to represent a message in written language) and instructions (for example, a procedure for moving a message from the bits its currently stored on to another set of bits). Building on this contept, you can arrive at the idea of a _programmable computer_.
>
> Now, let's say you have a computer, and your friend has a computer in another distant village. You have a message stored on your computer that you want to send to your friend. You could connect your computers with a long wire, which is made of a material that can turn "off" or "on", just like bits. Since your computers are programmable, you can write instructions to take the message on your computer and flash the wire "on" and "off" to "read" out the message on your computer. Your friend's computer would have a similar program to "read" the wire and copy the pattern of "off"s and "on"s onto its own bits. Now, in essence, you have the internet!

Obviously, there are tweaks that could be made. For example, I left out the idea of keyboards and monitors, and I didn't mention how wires can be replaced with wireless technology, but the core idea is there. This description also doesn't include mention of electricity, which I think is acceptable because the exact mechanics of electricity is :hook[low-level enough to be an implementation detail of computers and the internet]{id="implementation-detail"}.

I'm not sure if I even have to say this, but the odds that a Neolithic person living in Neolithic times could make these conceptual breakthroughs is _astronomically_ unlikely, and I'm even tempted to claim that it's categorically _impossible_. That, even if you simulated infinitely many neolithic agricultural societies, there wouldn't be _one_ human that could conceptualize the idea of the internet in this way. I don't have any hard facts to back up that stronger claim, but if you don't believe me, try it and get back to me.

I mean, the _second step_ in that sequence– recognizing that symbols can be used to represent spoken language– took about another six _thousand_ years from the Stone Age to work out. While we can't say for sure that _no one_ had the idea of written language, six _thousand_ years without evidence of writing sure says something.

:hook[Ideas follow the laws of natural selection]{id="memetics"}, and as such, complex concepts just take _time_ and _generations_ to pop into existence. Just as :hook[humans didn't suddenly pop into existence]{id="evolution"} in a vacuum, but evolved over millions of years of natural selection, the thought of an internet of computers was developed gradually over thousands of years, each generation of great minds standing atop the shoulders of the society that came before.

I think about Isaac Newton a lot in the context of being ahead of one's time. Newton's genius was pretty hard to understate. Over the course of his eight decades, he:

- Generalized the binomial theorem from positive integers to all real numbers
- Invented the reflecting telescope
- Invented calculus
- Published _Principia Mathematica_, which laid out the laws of motion _and_ the idea of universal gravitation
- Advanced statistical analysis with his method of least squares
- Made the first attempt to reconcile the wave- and particle-like properties of light
- Anticipated the idea of an electric field
- Refined the scientific method

And a whole lot more that I probably forgot. But for all his genius, he probably couldn't have been able to formulate quantum field theory, or general relativity, or predict the existence of black holes, or prove Fermat's Last Theorem. The stage was set, so to speak, for Newton to help push humanity from the Scientific Revolution into the Enlightenment, but the time was not right for modern physics or ring theory. Genius only gets you so far. So, assuming humans continue to exist in 10,000 years (which I highly doubt), the concepts familiar to our descendants in the year 22025 would be truly unthinkable to us now, with our current understanding of the universe.

That sums up _one_ potential answer to the question of, "Are there unthinkable thoughts?", which is, "Yes, if the concepts require too much evolution from current concepts". Now, let's take a look at a different angle: experiences that our brains just aren't wired to imagine.

## Brain software and hardware

Most computers we’re familiar with have both hardware and software. The hardware includes the physical components that literally make up the computer, like the CPU, RAM, disk, motherboard, and peripherals like the monitor and speakers. We can also write out sets of instructions, or programs, that tell the hardware what to do. These programs constitute the software of the computer. In this way, two computers with the same hardware can be programmed to do extremely different things.

In addition to distinguishing between hardware and software, you could identify multiple _layers_ of software in a computer, ranging from "low-level" (close to the hardware) to "high-level" (abstracted from the hardware). The lowest layer would probably be the assembler, which turns :hook[assembly language]{id="assembly"} into `1`s and `0`s that the CPU can execute. The next lowest level software above that would be the compiler, which turns human-readable code into assembly, and somewhere above that, we’d find the operating system, which is the most user-friendly way to interface with the computer. These higher levels of software make the hardware easier to work with at the expense of flexibility. For example, the operating system won’t let you write so many files that you overwrite the memory containing the instructions for the operating system itself, even though the hardware technically permits it.

The brain can be understood similarly. The "hardware" of our brain would be our neurons, as well as the laws of physics that govern the chemistry that governs those neurons. Dependent on that are various (fuzzy) layers of mental "software". Obviously, these different mental levels of cognition can't be teased apart as easily as the CPU from the motherboard, but some brain functions certainly seem lower-level than others. For example, there are parts of your brain (notably the brain stem) that are responsible for regulating your heart rate and breathing rate. These are very low-level functions. You can become conscious of your breathing and :hook[choose to hold your breath longer than average]{id="breath"} ("you" referring to the part of your brain responsible for your high-level sense of self and agency), but ultimately, your brain stem has the final say and will take over to prevent you from dying of hypoxia.

Cutting to the chase: while you can change the software of your brain through conscious thought and experiences, there's also mental hardware, which is much less malleable. Just like you can’t program a regular computer to be a quantum computer, you can’t think your way into, say, conceptualizing the fourth dimension.

Let's elaborate on that.

### The fourth dimension?

Consider your eyes. The eye gets a (more or less) two-dimensional image of the world. I say that because an eye perceives light and, unlike radar, there's no way for the eye to know how _far_ that light traveled to reach it. All our brain has to work with are neural signals from a field of cones that get stimulated by different wavelengths of light. To make matters even more two-dimensional, our eyes rest only a few inches apart on the plane of our face, meaning we can usually only see at most _half_ of an object at one given time.

So, if we've never seen every side of an object at once, why do we experience the world as a three-dimensional place with three-dimensional things, instead of a two-dimensional screen of moving pictures? It goes beyond the fact that we have :hook[two eyes]{id="two-eyes"}; the visual processing part of our brain is hard-wired to use certain visual cues to construct a three-dimensional model of the world from these :hook[excited cones]{id="band"}. This automatic "lifting" of visual information to three dimensions allows us to look at a video, or an image, or even a drawing, and perceive depth with no effort necessary. Quite the opposite: it's basically impossible to _not_ see things in three dimensions. That said, let's try to see things in four dimensions.

#### Picturing the fourth dimension

We can actually learn a lot of things about the fourth dimension just by making connections between lower dimensions. I’ll go through a few of them.

As three dimensional beings, we can look at a 2D object and see _all_ its sides at once, including the inside of the object. It's all "laid out in front of us". In our three-dimensional world, a single viewpoint is restricted to seeing at most half of an object at a time, and the inside is hidden from view. In the fourth dimension, then, we could look at a 3D object and easily see _all_ of its sides at once, as well as its contents. Picture a box in front of you, with things inside. If you were a fourth-dimensional being, you could see all six sides of the box, as well as the contents of the box. You could also see every side of each item in the box, and their insides. It would all be laid out in front of you.

We can discover some additional details by generalizing familiar geometry. For instance, the amount of space taken up by a line is just the length of the line. The amount of space taken up by a square is its area, which is the side length squared. Similarly, the volume of a cube is the edge length cubed. Generalizing further, the _hypervolume_ of a tesseract (a four-dimensional cube) would be the edge length raised to the fourth power.

Here's another fact: a line has 2 sides, a square has 4, and a cube has 6 faces, so a tesseract would have 8 "faces", or _cells_ (as they're called).

One more: if you picked two endpoints of a line (there’s only one choice), they would be separated by one edge, namely, the line itself. On a square, the most edges you could find between any two vertices is two. On a cube, that number is three. Consequently, on a tesseract, the most number of edges separating two vertices would be four.

Surely, given all this information, you could picture a tesseract in all its four-dimensional glory?

Clearly, no amount of describing the properties of a fourth spatial dimension would enable us to _actually_ conceptualize it with the same level of vividness that we can grasp the third dimension. Although some people have dedicated a lot of time to the pursuit of fourth-dimensional intuition, I would argue that these people are dancing _around_ four-dimensional thoughts, as closely as possible without actually experiencing it. You could, like I just did, read through the [Wikipedia page for Tesseract](https://en.wikipedia.org/wiki/Tesseract) and go like "uh huh, yep, makes sense", but actual four-dimensional thoughts would still remain just out of reach.

This is categorically different from things like cows jumping over the moon, which we also have no experience of. It's not just that we've never experienced the fourth dimension, it's that the _hardware_ of our brain– which went all-in on the third dimension– is working directly against us. You could make a similar argument for other sensations that humans don't experience, like seeing infrared / ultraviolet light, or sensing magnetic fields. But I think higher dimensions are a much more satisfying candidate for "unthinkable thoughts" because they're _so close_ to our regular thoughts, and at the same time, so far away that the word "far" doesn't even apply.

> **Addendum**: After writing this essay and coming back to this section, I've softened my position a little on the subject of whether higher dimensions are possible to fully conceptualize. I think my main point mostly stands: that your brain has never perceived _anything_ four-dimensional, which makes it fundamentally different from the other "impossible" scenarios we can construct in our heads. But we do have experience with time, which is a good starting point for understanding four spatial dimensions, and I think it's possible to amass so much intuition about the fourth dimension that it feels like you fully grasp it (at least, as fully as a 3-human can). So, pick your definition of "thought".

## Unreachable levels of abstraction

I'd like to call your attention to a wonderful little principle called the pigeonhole principle. If you're not aware of it, this is essentially what it says:

> **Pigeonhole principle**: Imagine you have `N` things and `M` containers. If `N > M`, then at least one container will have more than one thing in it. Similarly, if `N < M`, then at least one container will have no things in it.

You could prove this fact from first principles, but there's really no need, since it's so obviously _true_. Even though the principle itself is simple, its power comes from its generality. For example, you can use it to prove that there must be two people in London who have the same number of hairs on their head, or that, in a situation where people are shaking hands, there will always be at least two people who have :hook[shaken the same number of hands]{id="shake"}.

Let's go back to our echinodermic friend, the starfish, with its 500 neurons. Could a starfish ever conceptualize the pigeonhole principle? I strongly doubt it. Even if a particular starfish got lots of experience putting things in containers (already doubtful), the idea that it would generalize those experiences to a "principle", with all of its 500 neurons, is fantasy. I don't know where the line is, but it's above 500.

Let's jump way up the neurological pecking order and consider chimpanzees. Could a _chimp_ conceptualize the pigeonhole principle? It feels harder to say "no" right away. I'm certain that a chimp understands the concept of "you can't fit too many things in a container". But the pigeonhole principle deals with an idea more discrete and precise than that, and I'm not immediately sure that a :hook[chimp can really understand]{id="chimp"} the general principle. Maybe they can, maybe they can't.

So, on the scale of "understands the pigeonhole principle", we definitely have a spectrum from starfish to humans. This raises an obvious question: why? Although having a sufficient number of neurons is definitely a prerequisite, it's clearly not _the_ deciding factor. Going back to the "symbolic thought" I mentioned in the Stone Age polymath example, humans' capacity for _symbolic_ thought– abstracting patterns from reality, generalizing those patterns, and applying patterns in new contexts– _seems_ pretty unique, and it probably coevolved with our evolutionary need for advanced language.

This should give us some real confidence that humans have "won the game" in terms of intelligence, and are capable of conceptualizing any abstract concept or pattern. And while I can't :hook[prove otherwise]{id="prove-otherwise"}, there's something that makes me suspect that we don't have the final say.

:footnote[I'm putting "real" in quotes because in the context of our perception, everything is subjective. There's no separate compartments in your brain for the stuff that "really" happened and the stuff you read in books or saw in movies. Sometimes it's hard to tell them apart.]{id="real"}
:footnote[Yes, you could "experience" such impossible things through things like computer graphics or animations, but those are just proof that someone else thought of the impossible thing first.]{id="possibly-experience"}
:footnote[All of a sudden, I got curious about what _first_ nature is supposed to be. Interestingly, it turns out that "second" here actually refers to "following" nature, kind of like how you might "second" an opinion. It comes from the latin _secundum naturum_, and is contrasted with _super naturum_ ("above nature") and _contra naturum_ ("against nature").]{id="second-nature"}
:footnote[Sometimes the connection between the symbol and thing is very clear, like "rock", but other times it's much more tenuous, like "beautification".]{id="loosely"}
:footnote[This sentence will get me into a lot of trouble.]{id="implementation-detail"}
:footnote[See: [memetics](https://en.wikipedia.org/wiki/Memetics), or this classic [CGP Grey video](https://www.youtube.com/watch?v=rE3j_RHkqJc).]{id="memetics"}
:footnote[Controversial opinion #2.]{id="evolution"}
:footnote[This statement led me to the Wikipedia page for [Diving reflex](https://en.wikipedia.org/wiki/Diving_reflex), which was a fascinating read. Turns out that our brain's hardware also includes instructions for subconsciously directing more blood to vital organs when our face is submerged and our nostrils fill with water. Crazy.]{id="breath"}
:footnote[Contrary to popular opinion, depth perception is not dependent on having two eyes; although there are many important visual cues that [do require them](https://en.wikipedia.org/wiki/Stereopsis), there are [plenty of others](https://www.wtamu.edu/~cbaird/sq/2023/07/28/why-does-a-person-with-only-one-working-eye-have-zero-depth-perception/) that don't.]{id="two-eyes"}
:footnote[Band name.]{id="band"}
:footnote[These are (barely) human-readable instructions for the CPU to execute, like `MOV EAX, 5`, which means to move the value of 5 into the register called `EAX`.]{id="assembly"}
:footnote[Proof: Suppose there's `N` people, or "pigeons". The "holes" in this analogy are the number of hands shaken. On first inspection, there seems to be `N` holes, since one can shake hands with between `0` and `N-1` people. But once you realize that it's impossible for one person to shake hands with everyone while somebody has shaken hands with no one, it's impossible for _both_ the `0` and `N-1` buckets to be occupied at the same time. Then, since there are `N` people and at most `N-1` available holes, by the pigeonhole principle, at least two people will have shaken the same number of hands.]{id="shake"}
:footnote[I spent way too long trying to think of an experimental design that could test this, and came up empty-handed. It seems hard to demonstrate that an animal understands something is impossible. If you have ideas, though, email me!]{id="chimp"}
:footnote[Not for lack of trying. I spent an embarassing number of hours sketching out an argument for why human intelligence has computational limits, but I realized about 1,000 words in that (a) there were some big holes in my argument and (b) it's all besides the point anyway.]{id="prove-otherwise"}
